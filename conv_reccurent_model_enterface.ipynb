{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Emotion Recognition on Enterface db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Import essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pylab as plt\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import imageio\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('data/jet/Male/subject 32'), WindowsPath('data/jet/Male/subject 39'), WindowsPath('data/jet/Male/subject 27'), WindowsPath('data/jet/Female/subject 33'), WindowsPath('data/jet/Male/subject 10'), WindowsPath('data/jet/Male/subject 24'), WindowsPath('data/jet/Male/subject 3'), WindowsPath('data/jet/Male/subject 11'), WindowsPath('data/jet/Male/subject 40'), WindowsPath('data/jet/Male/subject 34')]\n",
      "Collecting data: 43 of 43\n",
      "test data distribution:  [585. 482. 436. 394. 490. 392.]\n",
      "train data distribution:  [4127. 3204. 3217. 2673. 3757. 2864.]\n",
      "train input shape: (19842, 257, 257, 3)\n",
      "test inshape: (2779, 257, 257, 3)\n",
      "train output shape: (19842, 6)\n",
      "test output: (2779, 6)\n"
     ]
    }
   ],
   "source": [
    "data_dir = r\"data/\"\n",
    "\n",
    "img_x = 257\n",
    "img_y = 257\n",
    "input_shape = (img_x, img_y , 3)\n",
    "\n",
    "train_split = 0.9\n",
    "\n",
    "num_classes=6\n",
    "data_x = []\n",
    "data_y = []\n",
    "i = 1\n",
    "\n",
    "train_x = []\n",
    "train_y_labels = []\n",
    "\n",
    "test_x = []\n",
    "test_y_labels = []\n",
    "\n",
    "# collect sentences folders\n",
    "emotions_dirs = [\"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"]\n",
    "\n",
    "\n",
    "sentences_list = [dir for dir in Path(data_dir).glob('**/*') if dir.is_dir() and \"subject\" in dir.stem]\n",
    "sentences_list = shuffle(sentences_list, random_state=0)\n",
    "\n",
    "print(sentences_list[:10])\n",
    "\n",
    "train_sentences_list = sentences_list[:int(train_split * len(sentences_list))]\n",
    "test_sentences_list = sentences_list[int(train_split * len(sentences_list)):]\n",
    "\n",
    "for dir in train_sentences_list:\n",
    "    print(\"\\rCollecting data: {} of {}\".format(i, len(sentences_list)), end=\"\")\n",
    "    i += 1\n",
    "    for filename in glob.iglob(str(dir) + '/**/*.png', recursive=True):\n",
    "        im = imageio.imread(filename)\n",
    "        train_x.append(im)\n",
    "        train_y_labels.append(pathlib.Path(filename).parent.stem)\n",
    "\n",
    "\n",
    "for dir in test_sentences_list:\n",
    "    print(\"\\rCollecting data: {} of {}\".format(i, len(sentences_list)), end=\"\")\n",
    "    i += 1\n",
    "    for filename in glob.iglob(str(dir) + '/**/*.png', recursive=True):\n",
    "        im = imageio.imread(filename)\n",
    "        test_x.append(im)\n",
    "        test_y_labels.append(pathlib.Path(filename).parent.stem)\n",
    "# for filename in glob.iglob(data_dir + '**/*.png', recursive=True):\n",
    "#     print(\"\\rCollecting data: {}\".format(i), end=\"\")\n",
    "#     im = imageio.imread(filename)\n",
    "#     data_x.append(im)\n",
    "#     data_y.append(pathlib.Path(filename).parent.stem)\n",
    "    \n",
    "#     if i > 1000:\n",
    "#         break\n",
    "#     i += 1\n",
    "# print(\" finished\")\n",
    "# data_x = np.array(data_x)\n",
    "# data_y = np.array(data_y)\n",
    "\n",
    "# data_x, data_y = shuffle(data_x, data_y, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "# train_x = data_x[:int(train_split * data_x.shape[0])] \n",
    "# train_y_labels = data_y[:int(train_split * data_x.shape[0])]\n",
    "label_encoder = LabelEncoder()\n",
    "train_y_integer_encoded = label_encoder.fit_transform(train_y_labels)\n",
    "train_y = keras.utils.to_categorical(train_y_integer_encoded, num_classes)\n",
    "\n",
    "# test_x = data_x[int(train_split * data_x.shape[0]):]\n",
    "# test_y_labels = data_y[int(train_split * data_x.shape[0]):]\n",
    "test_y_integer_encoded = label_encoder.fit_transform(test_y_labels)\n",
    "test_y = keras.utils.to_categorical(test_y_integer_encoded, num_classes)\n",
    "\n",
    "train_x = np.array(train_x) / 255\n",
    "test_x = np.array(test_x) / 255\n",
    "\n",
    "print(\"\\ntest data distribution: \", np.sum(test_y, axis=0))\n",
    "print(\"train data distribution: \", np.sum(train_y, axis=0))\n",
    "\n",
    "print(\"train input shape: {}\".format(train_x.shape))\n",
    "print(\"test inshape: {}\".format(test_x.shape))\n",
    "\n",
    "print(\"train output shape: {}\".format(train_y.shape))\n",
    "print(\"test output: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model: conv --> dropout --> maxpool --> conv --> dropout --> dense --> dense\n",
    "# first convolutional layer --> dropout --> max pool \n",
    "first_filter_width = 8\n",
    "first_filter_height = 20\n",
    "first_filter_count = 64\n",
    "first_strides = [1, 1]\n",
    "first_conv = Conv2D(first_filter_count \n",
    "                    , kernel_size=(first_filter_height, first_filter_width)\n",
    "                    , strides=first_strides\n",
    "                    , padding='same'\n",
    "                    , activation='relu'\n",
    "                    , input_shape=input_shape)\n",
    " \n",
    "\n",
    "dropout_prob=0.5\n",
    "first_dropout = Dropout(rate=dropout_prob)\n",
    "\n",
    "pooling_size=2\n",
    "pooling_strides=2\n",
    "first_max_pool = MaxPooling2D(pool_size=pooling_size\n",
    "                              , strides=pooling_strides)\n",
    "\n",
    "# second conv layer --> dropout\n",
    "second_filter_width = 4\n",
    "second_filter_height = 10\n",
    "second_filter_count = 64\n",
    "second_strides = [1, 1]\n",
    "second_conv = Conv2D(second_filter_count \n",
    "                    , kernel_size=(second_filter_height, second_filter_width)\n",
    "                    , strides=second_strides\n",
    "                    , padding='same'\n",
    "                    , activation='relu')\n",
    "\n",
    "second_dropout=Dropout(rate=dropout_prob)\n",
    "\n",
    "third_units=500\n",
    "third_dense = Dense(units=third_units\n",
    "                   , activation='relu')\n",
    "\n",
    "fourth_dense = Dense(units=num_classes\n",
    "                    , activation='softmax')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(keras.layers.AlphaDropout(0.2, noise_shape=None, seed=None, input_shape=input_shape))\n",
    "model.add(keras.layers.GaussianNoise(0.2))\n",
    "model.add(first_conv)\n",
    "model.add(first_dropout)\n",
    "model.add(first_max_pool)\n",
    "# model.add(Conv2D(first_filter_count \n",
    "#                     , kernel_size=(first_filter_height, first_filter_width)\n",
    "#                     , strides=first_strides\n",
    "#                     , padding='same'\n",
    "#                     , activation='relu'\n",
    "#                     , input_shape=input_shape))\n",
    "# model.add(Dropout(rate=dropout_prob))\n",
    "# model.add(MaxPooling2D(pool_size=pooling_size\n",
    "#                               , strides=pooling_strides))\n",
    "\n",
    "# model.add(Conv2D(128 \n",
    "#                     , kernel_size=(10, 4)\n",
    "#                     , strides=first_strides\n",
    "#                     , padding='same'\n",
    "#                     , activation='relu'\n",
    "#                     , input_shape=input_shape))\n",
    "# model.add(Dropout(rate=dropout_prob))\n",
    "# model.add(MaxPooling2D(pool_size=pooling_size\n",
    "#                               , strides=pooling_strides))\n",
    "\n",
    "model.add(Conv2D(256 \n",
    "                    , kernel_size=(10, 4)\n",
    "                    , strides=first_strides\n",
    "                    , padding='same'\n",
    "                    , activation='relu'\n",
    "                    , input_shape=input_shape))\n",
    "model.add(Dropout(rate=dropout_prob))\n",
    "model.add(MaxPooling2D(pool_size=pooling_size\n",
    "                              , strides=pooling_strides))\n",
    "\n",
    "\n",
    "\n",
    "model.add(second_conv)\n",
    "model.add(second_dropout)\n",
    "model.add(Flatten())\n",
    "model.add(third_dense)\n",
    "model.add(Dense(units=200\n",
    "                , activation='relu'))\n",
    "model.add(fourth_dense)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainValTensorBoard(keras.callbacks.TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19842 samples, validate on 2779 samples\n",
      "Epoch 1/40\n",
      "19842/19842 [==============================] - 506s 26ms/step - loss: 1.6667 - acc: 0.3168 - val_loss: 1.7221 - val_acc: 0.3897\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.38971, saving model to E:/ckpnt/weights-improvement-01-0.39.hdf5\n",
      "Epoch 2/40\n",
      "19842/19842 [==============================] - 495s 25ms/step - loss: 1.4356 - acc: 0.4352 - val_loss: 1.6350 - val_acc: 0.3904\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.38971 to 0.39043, saving model to E:/ckpnt/weights-improvement-02-0.39.hdf5\n",
      "Epoch 3/40\n",
      "19842/19842 [==============================] - 494s 25ms/step - loss: 1.2389 - acc: 0.5272 - val_loss: 1.5692 - val_acc: 0.4397\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.39043 to 0.43973, saving model to E:/ckpnt/weights-improvement-03-0.44.hdf5\n",
      "Epoch 4/40\n",
      "19842/19842 [==============================] - 501s 25ms/step - loss: 1.0226 - acc: 0.6164 - val_loss: 1.5179 - val_acc: 0.4304\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/40\n",
      "19842/19842 [==============================] - 500s 25ms/step - loss: 0.7690 - acc: 0.7150 - val_loss: 1.3914 - val_acc: 0.4797\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.43973 to 0.47967, saving model to E:/ckpnt/weights-improvement-05-0.48.hdf5\n",
      "Epoch 6/40\n",
      "19842/19842 [==============================] - 500s 25ms/step - loss: 0.5340 - acc: 0.8049 - val_loss: 1.4328 - val_acc: 0.4372\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/40\n",
      "19842/19842 [==============================] - 500s 25ms/step - loss: 0.3736 - acc: 0.8632 - val_loss: 1.3959 - val_acc: 0.4764\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/40\n",
      "19842/19842 [==============================] - 500s 25ms/step - loss: 0.2822 - acc: 0.8976 - val_loss: 1.4577 - val_acc: 0.4401\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/40\n",
      "19842/19842 [==============================] - 500s 25ms/step - loss: 0.2300 - acc: 0.9200 - val_loss: 1.5108 - val_acc: 0.4498\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/40\n",
      "19842/19842 [==============================] - 495s 25ms/step - loss: 0.1839 - acc: 0.9343 - val_loss: 1.4614 - val_acc: 0.4584\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 11/40\n",
      "19842/19842 [==============================] - 493s 25ms/step - loss: 0.1675 - acc: 0.9384 - val_loss: 1.4927 - val_acc: 0.4674\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 12/40\n",
      "19842/19842 [==============================] - 491s 25ms/step - loss: 0.1527 - acc: 0.9449 - val_loss: 1.5208 - val_acc: 0.4646\n",
      "\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 13/40\n",
      "19842/19842 [==============================] - 496s 25ms/step - loss: 0.1342 - acc: 0.9543 - val_loss: 1.5117 - val_acc: 0.4696\n",
      "\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 14/40\n",
      "19842/19842 [==============================] - 495s 25ms/step - loss: 0.1213 - acc: 0.9557 - val_loss: 1.6063 - val_acc: 0.4563\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/40\n",
      "19842/19842 [==============================] - 494s 25ms/step - loss: 0.1124 - acc: 0.9607 - val_loss: 1.5957 - val_acc: 0.4664\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 16/40\n",
      "19842/19842 [==============================] - 496s 25ms/step - loss: 0.1031 - acc: 0.9634 - val_loss: 1.5540 - val_acc: 0.4487\n",
      "\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 17/40\n",
      "19842/19842 [==============================] - 496s 25ms/step - loss: 0.1026 - acc: 0.9642 - val_loss: 1.5218 - val_acc: 0.4743\n",
      "\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 18/40\n",
      "19842/19842 [==============================] - 492s 25ms/step - loss: 0.0900 - acc: 0.9675 - val_loss: 1.5650 - val_acc: 0.4714\n",
      "\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 19/40\n",
      "19840/19842 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9682"
     ]
    }
   ],
   "source": [
    "batch_size = 40\n",
    "epochs = 40\n",
    "\n",
    "logdir = \"_tf_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "tb = TrainValTensorBoard(log_dir=logdir)\n",
    "\n",
    "chkpnt_dir = \"E:/ckpnt/\"\n",
    "filepath= chkpnt_dir +\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "#           validation_split=0.2,\n",
    "          validation_data=(test_x, test_y),\n",
    "          callbacks=[tb, checkpoint])\n",
    "score = model.evaluate(test_x, test_y, verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6779497e-14 1.5544706e-16 0.0000000e+00 5.2727147e-23 2.7790000e+03\n",
      " 6.1526601e-15]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_x)\n",
    "print(np.sum(pred, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "y_train shape: (60000, 10)\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 16s 290us/step - loss: 0.1709 - acc: 0.9478 - val_loss: 0.0617 - val_acc: 0.9838\n",
      "Epoch 2/10\n",
      "38016/54000 [====================>.........] - ETA: 4s - loss: 0.0606 - acc: 0.9813"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_x, img_y = 28, 28\n",
    "\n",
    "# load the MNIST data set, which already splits into train and test sets for us\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape the data into a 4D tensor - (sample_number, x_img_size, y_img_size, num_channels)\n",
    "# because the MNIST is greyscale, we only have a single channel - RGB colour images would have 3\n",
    "x_train = x_train.reshape(x_train.shape[0], img_x, img_y, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_x, img_y, 1)\n",
    "input_shape = (img_x, img_y, 1)\n",
    "\n",
    "# convert the data to the right type\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices - this is for use in the\n",
    "# categorical_crossentropy loss below\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "# model: conv --> dropout --> maxpool --> conv --> dropout --> dense --> dense\n",
    "# first convolutional layer --> dropout --> max pool \n",
    "first_filter_width = 8\n",
    "first_filter_height = 20\n",
    "first_filter_count = 64\n",
    "first_strides = [1, 1]\n",
    "\n",
    "first_conv = Conv2D(first_filter_count \n",
    "                    , kernel_size=(first_filter_height, first_filter_width)\n",
    "                    , strides=first_strides\n",
    "                    , padding='same'\n",
    "                    , activation='relu'\n",
    "                    , input_shape=input_shape)\n",
    " \n",
    "\n",
    "dropout_prob=0.5\n",
    "first_dropout = Dropout(rate=dropout_prob)\n",
    "\n",
    "pooling_size=2\n",
    "pooling_strides=2\n",
    "first_max_pool = MaxPooling2D(pool_size=pooling_size\n",
    "                              , strides=pooling_strides)\n",
    "\n",
    "# second conv layer --> dropout\n",
    "second_filter_width = 4\n",
    "second_filter_height = 10\n",
    "second_filter_count = 64\n",
    "second_strides = [1, 1]\n",
    "second_conv = Conv2D(second_filter_count \n",
    "                    , kernel_size=(second_filter_height, second_filter_width)\n",
    "                    , strides=second_strides\n",
    "                    , padding='same'\n",
    "                    , activation='relu')\n",
    "\n",
    "second_dropout=Dropout(rate=dropout_prob)\n",
    "\n",
    "third_units=200\n",
    "third_dense = Dense(units=third_units\n",
    "                   , activation='relu')\n",
    "\n",
    "fourth_dense = Dense(units=num_classes\n",
    "                    , activation='softmax')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(first_conv)\n",
    "model.add(first_dropout)\n",
    "model.add(first_max_pool)\n",
    "model.add(second_conv)\n",
    "model.add(second_dropout)\n",
    "model.add(Flatten())\n",
    "model.add(third_dense)\n",
    "model.add(fourth_dense)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
