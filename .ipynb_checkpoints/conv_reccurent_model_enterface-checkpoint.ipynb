{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Emotion Recognition on Enterface db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Import essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pylab as plt\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import imageio\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('data/jet/Male/subject 32'), WindowsPath('data/jet/Male/subject 39'), WindowsPath('data/jet/Male/subject 27'), WindowsPath('data/jet/Female/subject 33'), WindowsPath('data/jet/Male/subject 10'), WindowsPath('data/jet/Male/subject 24'), WindowsPath('data/jet/Male/subject 3'), WindowsPath('data/jet/Male/subject 11'), WindowsPath('data/jet/Male/subject 40'), WindowsPath('data/jet/Male/subject 34')]\n",
      "Collecting data: 43 of 43\n",
      "test data distribution:  [585. 482. 436. 394. 490. 392.]\n",
      "train data distribution:  [4127. 3204. 3217. 2673. 3757. 2864.]\n",
      "train input shape: (19842, 257, 257, 3)\n",
      "test inshape: (2779, 257, 257, 3)\n",
      "train output shape: (19842, 6)\n",
      "test output: (2779, 6)\n"
     ]
    }
   ],
   "source": [
    "data_dir = r\"data/\"\n",
    "\n",
    "img_x = 257\n",
    "img_y = 257\n",
    "input_shape = (img_x, img_y , 3)\n",
    "\n",
    "train_split = 0.9\n",
    "\n",
    "num_classes=6\n",
    "data_x = []\n",
    "data_y = []\n",
    "i = 1\n",
    "\n",
    "train_x = []\n",
    "train_y_labels = []\n",
    "\n",
    "test_x = []\n",
    "test_y_labels = []\n",
    "\n",
    "# collect sentences folders\n",
    "emotions_dirs = [\"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"]\n",
    "\n",
    "\n",
    "sentences_list = [dir for dir in Path(data_dir).glob('**/*') if dir.is_dir() and \"subject\" in dir.stem]\n",
    "sentences_list = shuffle(sentences_list, random_state=0)\n",
    "\n",
    "print(sentences_list[:10])\n",
    "\n",
    "train_sentences_list = sentences_list[:int(train_split * len(sentences_list))]\n",
    "test_sentences_list = sentences_list[int(train_split * len(sentences_list)):]\n",
    "\n",
    "for dir in train_sentences_list:\n",
    "    print(\"\\rCollecting data: {} of {}\".format(i, len(sentences_list)), end=\"\")\n",
    "    i += 1\n",
    "    for filename in glob.iglob(str(dir) + '/**/*.png', recursive=True):\n",
    "        im = imageio.imread(filename)\n",
    "        train_x.append(im)\n",
    "        train_y_labels.append(pathlib.Path(filename).parent.stem)\n",
    "\n",
    "\n",
    "for dir in test_sentences_list:\n",
    "    print(\"\\rCollecting data: {} of {}\".format(i, len(sentences_list)), end=\"\")\n",
    "    i += 1\n",
    "    for filename in glob.iglob(str(dir) + '/**/*.png', recursive=True):\n",
    "        im = imageio.imread(filename)\n",
    "        test_x.append(im)\n",
    "        test_y_labels.append(pathlib.Path(filename).parent.stem)\n",
    "# for filename in glob.iglob(data_dir + '**/*.png', recursive=True):\n",
    "#     print(\"\\rCollecting data: {}\".format(i), end=\"\")\n",
    "#     im = imageio.imread(filename)\n",
    "#     data_x.append(im)\n",
    "#     data_y.append(pathlib.Path(filename).parent.stem)\n",
    "    \n",
    "#     if i > 1000:\n",
    "#         break\n",
    "#     i += 1\n",
    "# print(\" finished\")\n",
    "# data_x = np.array(data_x)\n",
    "# data_y = np.array(data_y)\n",
    "\n",
    "# data_x, data_y = shuffle(data_x, data_y, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "# train_x = data_x[:int(train_split * data_x.shape[0])] \n",
    "# train_y_labels = data_y[:int(train_split * data_x.shape[0])]\n",
    "label_encoder = LabelEncoder()\n",
    "train_y_integer_encoded = label_encoder.fit_transform(train_y_labels)\n",
    "train_y = keras.utils.to_categorical(train_y_integer_encoded, num_classes)\n",
    "\n",
    "# test_x = data_x[int(train_split * data_x.shape[0]):]\n",
    "# test_y_labels = data_y[int(train_split * data_x.shape[0]):]\n",
    "test_y_integer_encoded = label_encoder.fit_transform(test_y_labels)\n",
    "test_y = keras.utils.to_categorical(test_y_integer_encoded, num_classes)\n",
    "\n",
    "train_x = np.array(train_x) / 255\n",
    "test_x = np.array(test_x) / 255\n",
    "\n",
    "print(\"\\ntest data distribution: \", np.sum(test_y, axis=0))\n",
    "print(\"train data distribution: \", np.sum(train_y, axis=0))\n",
    "\n",
    "print(\"train input shape: {}\".format(train_x.shape))\n",
    "print(\"test inshape: {}\".format(test_x.shape))\n",
    "\n",
    "print(\"train output shape: {}\".format(train_y.shape))\n",
    "print(\"test output: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model: conv --> dropout --> maxpool --> conv --> dropout --> dense --> dense\n",
    "# first convolutional layer --> dropout --> max pool \n",
    "first_filter_width = 8\n",
    "first_filter_height = 20\n",
    "first_filter_count = 64\n",
    "first_strides = [1, 1]\n",
    "first_conv = Conv2D(first_filter_count \n",
    "                    , kernel_size=(first_filter_height, first_filter_width)\n",
    "                    , strides=first_strides\n",
    "                    , padding='same'\n",
    "                    , activation='relu'\n",
    "                    , input_shape=input_shape)\n",
    " \n",
    "\n",
    "dropout_prob=0.5\n",
    "first_dropout = Dropout(rate=dropout_prob)\n",
    "\n",
    "pooling_size=2\n",
    "pooling_strides=2\n",
    "first_max_pool = MaxPooling2D(pool_size=pooling_size\n",
    "                              , strides=pooling_strides)\n",
    "\n",
    "# second conv layer --> dropout\n",
    "second_filter_width = 4\n",
    "second_filter_height = 10\n",
    "second_filter_count = 64\n",
    "second_strides = [1, 1]\n",
    "second_conv = Conv2D(second_filter_count \n",
    "                    , kernel_size=(second_filter_height, second_filter_width)\n",
    "                    , strides=second_strides\n",
    "                    , padding='same'\n",
    "                    , activation='relu')\n",
    "\n",
    "second_dropout=Dropout(rate=dropout_prob)\n",
    "\n",
    "third_units=500\n",
    "third_dense = Dense(units=third_units\n",
    "                   , activation='relu')\n",
    "\n",
    "fourth_dense = Dense(units=num_classes\n",
    "                    , activation='softmax')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(keras.layers.AlphaDropout(0.2, noise_shape=None, seed=None, input_shape=input_shape))\n",
    "model.add(keras.layers.GaussianNoise(0.2))\n",
    "model.add(first_conv)\n",
    "model.add(first_dropout)\n",
    "model.add(first_max_pool)\n",
    "# model.add(Conv2D(first_filter_count \n",
    "#                     , kernel_size=(first_filter_height, first_filter_width)\n",
    "#                     , strides=first_strides\n",
    "#                     , padding='same'\n",
    "#                     , activation='relu'\n",
    "#                     , input_shape=input_shape))\n",
    "# model.add(Dropout(rate=dropout_prob))\n",
    "# model.add(MaxPooling2D(pool_size=pooling_size\n",
    "#                               , strides=pooling_strides))\n",
    "\n",
    "# model.add(Conv2D(128 \n",
    "#                     , kernel_size=(10, 4)\n",
    "#                     , strides=first_strides\n",
    "#                     , padding='same'\n",
    "#                     , activation='relu'\n",
    "#                     , input_shape=input_shape))\n",
    "# model.add(Dropout(rate=dropout_prob))\n",
    "# model.add(MaxPooling2D(pool_size=pooling_size\n",
    "#                               , strides=pooling_strides))\n",
    "\n",
    "# model.add(Conv2D(256 \n",
    "#                     , kernel_size=(10, 4)\n",
    "#                     , strides=first_strides\n",
    "#                     , padding='same'\n",
    "#                     , activation='relu'\n",
    "#                     , input_shape=input_shape))\n",
    "# model.add(Dropout(rate=dropout_prob))\n",
    "# model.add(MaxPooling2D(pool_size=pooling_size\n",
    "#                               , strides=pooling_strides))\n",
    "\n",
    "\n",
    "\n",
    "model.add(second_conv)\n",
    "model.add(second_dropout)\n",
    "model.add(Flatten())\n",
    "# model.add(third_dense)\n",
    "model.add(Dense(units=200\n",
    "                , activation='relu'))\n",
    "model.add(fourth_dense)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainValTensorBoard(keras.callbacks.TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19842 samples, validate on 2779 samples\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[40,257,257,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: dropout_1/cond/dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=8129445, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dropout_1/cond/dropout/Shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: metrics/acc/Mean/_163 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_928_metrics/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'dropout_1/cond/dropout/random_uniform/RandomUniform', defined at:\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-93b5103845fd>\", line 48, in <module>\n    model.add(first_dropout)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 492, in add\n    output_tensor = layer(self.outputs[0])\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\", line 123, in call\n    training=training)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2879, in in_train_phase\n    x = switch(training, x, alt)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2814, in switch\n    else_expression_fn)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 432, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2018, in cond\n    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 1868, in BuildCondBranch\n    original_result = fn()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\", line 121, in dropped_inputs\n    seed=self.seed)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3138, in dropout\n    return tf.nn.dropout(x * 1., retain_prob, noise_shape, seed=seed)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 2272, in dropout\n    noise_shape, seed=seed, dtype=x.dtype)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\", line 244, in random_uniform\n    shape, dtype, seed=seed1, seed2=seed2)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_random_ops.py\", line 473, in _random_uniform\n    name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[40,257,257,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: dropout_1/cond/dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=8129445, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dropout_1/cond/dropout/Shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: metrics/acc/Mean/_163 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_928_metrics/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[40,257,257,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: dropout_1/cond/dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=8129445, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dropout_1/cond/dropout/Shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: metrics/acc/Mean/_163 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_928_metrics/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0de6d65a4530>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#           validation_split=0.2,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m           callbacks=[tb, checkpoint])\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[40,257,257,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: dropout_1/cond/dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=8129445, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dropout_1/cond/dropout/Shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: metrics/acc/Mean/_163 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_928_metrics/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'dropout_1/cond/dropout/random_uniform/RandomUniform', defined at:\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-93b5103845fd>\", line 48, in <module>\n    model.add(first_dropout)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 492, in add\n    output_tensor = layer(self.outputs[0])\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\", line 123, in call\n    training=training)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2879, in in_train_phase\n    x = switch(training, x, alt)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2814, in switch\n    else_expression_fn)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 432, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2018, in cond\n    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 1868, in BuildCondBranch\n    original_result = fn()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\", line 121, in dropped_inputs\n    seed=self.seed)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3138, in dropout\n    return tf.nn.dropout(x * 1., retain_prob, noise_shape, seed=seed)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 2272, in dropout\n    noise_shape, seed=seed, dtype=x.dtype)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\", line 244, in random_uniform\n    shape, dtype, seed=seed1, seed2=seed2)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_random_ops.py\", line 473, in _random_uniform\n    name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[40,257,257,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: dropout_1/cond/dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=8129445, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dropout_1/cond/dropout/Shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: metrics/acc/Mean/_163 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_928_metrics/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "batch_size = 40\n",
    "epochs = 40\n",
    "\n",
    "logdir = \"_tf_logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "tb = TrainValTensorBoard(log_dir=logdir)\n",
    "\n",
    "chkpnt_dir = \"E:/ckpnt/\"\n",
    "filepath= chkpnt_dir +\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "#           validation_split=0.2,\n",
    "          validation_data=(test_x, test_y),\n",
    "          callbacks=[tb, checkpoint])\n",
    "score = model.evaluate(test_x, test_y, verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(test_x)\n",
    "print(np.sum(pred, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "y_train shape: (60000, 10)\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 16s 290us/step - loss: 0.1709 - acc: 0.9478 - val_loss: 0.0617 - val_acc: 0.9838\n",
      "Epoch 2/10\n",
      "38016/54000 [====================>.........] - ETA: 4s - loss: 0.0606 - acc: 0.9813"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_x, img_y = 28, 28\n",
    "\n",
    "# load the MNIST data set, which already splits into train and test sets for us\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape the data into a 4D tensor - (sample_number, x_img_size, y_img_size, num_channels)\n",
    "# because the MNIST is greyscale, we only have a single channel - RGB colour images would have 3\n",
    "x_train = x_train.reshape(x_train.shape[0], img_x, img_y, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_x, img_y, 1)\n",
    "input_shape = (img_x, img_y, 1)\n",
    "\n",
    "# convert the data to the right type\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices - this is for use in the\n",
    "# categorical_crossentropy loss below\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "# model: conv --> dropout --> maxpool --> conv --> dropout --> dense --> dense\n",
    "# first convolutional layer --> dropout --> max pool \n",
    "first_filter_width = 8\n",
    "first_filter_height = 20\n",
    "first_filter_count = 64\n",
    "first_strides = [1, 1]\n",
    "\n",
    "first_conv = Conv2D(first_filter_count \n",
    "                    , kernel_size=(first_filter_height, first_filter_width)\n",
    "                    , strides=first_strides\n",
    "                    , padding='same'\n",
    "                    , activation='relu'\n",
    "                    , input_shape=input_shape)\n",
    " \n",
    "\n",
    "dropout_prob=0.5\n",
    "first_dropout = Dropout(rate=dropout_prob)\n",
    "\n",
    "pooling_size=2\n",
    "pooling_strides=2\n",
    "first_max_pool = MaxPooling2D(pool_size=pooling_size\n",
    "                              , strides=pooling_strides)\n",
    "\n",
    "# second conv layer --> dropout\n",
    "second_filter_width = 4\n",
    "second_filter_height = 10\n",
    "second_filter_count = 64\n",
    "second_strides = [1, 1]\n",
    "second_conv = Conv2D(second_filter_count \n",
    "                    , kernel_size=(second_filter_height, second_filter_width)\n",
    "                    , strides=second_strides\n",
    "                    , padding='same'\n",
    "                    , activation='relu')\n",
    "\n",
    "second_dropout=Dropout(rate=dropout_prob)\n",
    "\n",
    "third_units=200\n",
    "third_dense = Dense(units=third_units\n",
    "                   , activation='relu')\n",
    "\n",
    "fourth_dense = Dense(units=num_classes\n",
    "                    , activation='softmax')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(first_conv)\n",
    "model.add(first_dropout)\n",
    "model.add(first_max_pool)\n",
    "model.add(second_conv)\n",
    "model.add(second_dropout)\n",
    "model.add(Flatten())\n",
    "model.add(third_dense)\n",
    "model.add(fourth_dense)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
